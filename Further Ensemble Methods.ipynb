{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Ensemble Methods - Table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jypyter Notebook takes the first table, and runs ensemble methods to try to maximise model performance. The results of this will be used in the analyses of the report. Note that we already have 100% accuracy for table1, and so we cannot improve upon this by using more ensemble methods (note that random forests, which were chosen as the optimal model for table1, are an ensemble method themself). This notebook is produced to see whether model performance can be enhanced by running ensemble methods. In order to make a comparison between results, we do not use  use GridSearchCV, and instead try to see whether we can improve upon the standard results for several classifiers by using different ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary packages, and view a summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "# ignoring warnings, to make the results simpler to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.read_excel('Data1.xlsx', sheet_name = 'Table1 - Assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.set_index('Patient No. (ID)', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = new.drop(columns=['Output'])\n",
    "y = new.Output\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(y):\n",
    "    if y == 'Easy':\n",
    "        return 1\n",
    "    if y == 'Medium':\n",
    "        return 2\n",
    "    if y == 'Hard':\n",
    "        return 3\n",
    "y_train = y_train.apply(label_encoder)\n",
    "y_test = y_test.apply(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Shoulder abduction</th>\n",
       "      <th>Average Shoulder flexion</th>\n",
       "      <th>Average Elbow flexion</th>\n",
       "      <th>Total time taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient No. (ID)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>97</td>\n",
       "      <td>74</td>\n",
       "      <td>83</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>122</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>125</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>134</td>\n",
       "      <td>101</td>\n",
       "      <td>97</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Average Shoulder abduction  Average Shoulder flexion  \\\n",
       "Patient No. (ID)                                                         \n",
       "40                                        97                        74   \n",
       "12                                        56                        55   \n",
       "53                                       122                        91   \n",
       "56                                       125                        94   \n",
       "65                                       134                       101   \n",
       "\n",
       "                  Average Elbow flexion  Total time taken  \n",
       "Patient No. (ID)                                           \n",
       "40                                   83               101  \n",
       "12                                   57               121  \n",
       "53                                   92                94  \n",
       "56                                   94                93  \n",
       "65                                   97                89  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient No. (ID)\n",
       "22    1\n",
       "54    3\n",
       "21    1\n",
       "7     1\n",
       "28    2\n",
       "Name: Output, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging consists of making random samples of the dataset with replacement, and training a weak learner on each. A strong learner is then used, that is a combination of these weak learners. We firstly make a simple attempt at bagging, by applying Scikit-learn's BaggingClassifier() to the eight classification models seen in table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First accuracy for SVC: mean: 0.861888111888112, std: 0.12449603909768377\n",
      "Bagging accuracy for SVC: mean: 0.8452214452214453, std: 0.10957483861906214\n",
      "First accuracy for Logistic Regression: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "Bagging accuracy for Logistic Regression: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "First accuracy for LDA: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "Bagging accuracy for LDA: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "First accuracy for KNN: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "Bagging accuracy for KNN: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "First accuracy for Decision Tree: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "Bagging accuracy for Decision Tree: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "First accuracy for GaussianNB: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "Bagging accuracy for GaussianNB: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "First accuracy for Random Forest: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "Bagging accuracy for Random Forest: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "First accuracy for XGBoost: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "Bagging accuracy for XGBoost: mean: 0.9651515151515151, std: 0.04274768478686633\n"
     ]
    }
   ],
   "source": [
    "classifiers = [SVC(), LogisticRegression(), LinearDiscriminantAnalysis(), KNeighborsClassifier(), \n",
    "               DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(), XGBClassifier()]\n",
    "names = ['SVC', 'Logistic Regression', 'LDA', 'KNN', 'Decision Tree', 'GaussianNB', 'Random Forest', 'XGBoost']\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    first_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    bagging_classifier = BaggingClassifier(classifier)\n",
    "    bagging_score = cross_val_score(bagging_classifier, X_train, y_train, cv=5)\n",
    "    print(f'First accuracy for {names[i]}: mean: {first_score.mean()}, std: {first_score.std()}')\n",
    "    print(f'Bagging accuracy for {names[i]}: mean: {bagging_score.mean()}, std: {bagging_score.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, however, the accuracy scores remained relatively consistent. Indeed, the bagging accuracy only increased for XGBoost. The standard deviation also remained relatively consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now employ Scikit-learn's VotingClassifier(), which produces an ensemble of the eight other classifications (weak learners) by taking a 'vote' of the eight for their classification, and selecting the output that has a majority. That is, given an input, if five of the classifiers output Easy, the Ensemble classifier will choose the classification Easy, because it is selected by a majority of weak learners. We create this strong learner, called Ensemble, and compare it to the performance of the individual weak learners.\n",
    "\n",
    "Note that these weak learners have default hyperparameters, not optimised hyperparameters seen in the Table1 notebook. This is why the accuracy scores for the individual classification models are not 100%. This provides strong evidence that the hyperparameter optimisation used in Table1 and Table2 notebooks was working, and hence was a crucial step in optimally training each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVC: mean: 0.861888111888112, std: 0.12449603909768377\n",
      "Accuracy for Logistic Regression: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "Accuracy for LDA: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "Accuracy for KNN: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "Accuracy for Decision Tree: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "Accuracy for GaussianNB: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "Accuracy for Random Forest: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "Accuracy for XGBoost: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "Accuracy for Ensemble: mean: 0.9636363636363636, std: 0.07272727272727271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting = VotingClassifier(estimators = [(names[i], classifiers[i]) for i in range (8)], voting='hard')\n",
    "classifiers2 = [SVC(), LogisticRegression(), LinearDiscriminantAnalysis(), KNeighborsClassifier(), \n",
    "               DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(), XGBClassifier(), voting]\n",
    "names2 = ['SVC', 'Logistic Regression', 'LDA', 'KNN', 'Decision Tree', 'GaussianNB', 'Random Forest', 'XGBoost', 'Ensemble']\n",
    "for i, classifier in enumerate(classifiers2):\n",
    "    bagging_score2 = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(f'Accuracy for {names2[i]}: mean: {bagging_score2.mean()}, std: {bagging_score2.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy scores for Ensemble did significantly better, and had the joint lowest standard deviation. This shows that bagging ensemble methods, in taking a majority vote among multiple weak learners, is beneficial in training the models. Random Forests provide a very similar method of bagging to fit the data, and is one of the reasons why it performed best in both workbooks: bagging is a widely successful ensemble method in machine learning models, and is especially useful in the case of smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting, in contrast to bagging, works by iteratively creating multiple models using a weak learner. Each new model learns from the previous, and so a stronger model is created each time. In Table1 and Table2 XGBoost was used as the most popular boosting technique, from which we already have the results, and so no further analysis will be taken from Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking seeks to combine multiple classification methods via a meta-classifier, which is trained on the outputs, or meta-features, of the individual classification models in the ensemble. The report sought to copare stacking with the other two ensemble methods, and so this is what will happen here. The below code uses the itertools library to create every possible combination of three classifiers from the eight used above, which are used for the first layer of the stacking process. The outputs of these three classifiers are then taken by a meta-cassifier (in this case a random forest classifier, as that is what has achieved best results in Table1 and Table2), and outputs from this are used to predict the outputs of the training data. The accuracy scores from each combination of classifiers are taken using a mean of cross-validation scors, and displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations # importing modules\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "def stacking(no_stacked):\n",
    "    rf = RandomForestClassifier() # the meta-classifier to be applied to the outputs of the first layer\n",
    "    classifiers = [SVC(), LogisticRegression(), LinearDiscriminantAnalysis(), KNeighborsClassifier(), \n",
    "                   DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(), XGBClassifier()] # list of eight classifiers\n",
    "    names = ['SVC', 'Logistic Regression', 'LDA', 'KNN', 'Decision Tree', 'GaussianNB', 'Random Forest', 'XGBoost']\n",
    "    combined = {} # creates a blank dictoinary\n",
    "    for i, p in enumerate(names):\n",
    "        combined[p] = classifiers[i] # creating a dictionary of name: algorithm pairs\n",
    "    comb = list(combinations(combined.keys(), no_stacked)) # produces a list containing every possible combination of the eight\n",
    "    # classification algorithms, with number no_stacked algorithms in the first layer\n",
    "    for i in comb: # produces a stacking classifier on each combination of algorithms created in comb\n",
    "        stacking = StackingClassifier(classifiers=[combined[i[k]] for k in range(no_stacked)], meta_classifier=rf)\n",
    "        # the above line uses a list comprehension to obtain each combination created in comb\n",
    "        stacking_score = cross_val_score(stacking, X_train, y_train, cv=5) # uses cross-validation to produce an accuracy score\n",
    "        printing_names = [i[k] for k in range(no_stacked)] # lists the names of algorithms used in each combination\n",
    "        print(f'{printing_names} stacked: mean: {stacking_score.mean()}, std: {stacking_score.std()}') # prints the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SVC', 'Logistic Regression', 'LDA'] stacked: mean: 0.9331002331002332, std: 0.06258035841187393\n",
      "['SVC', 'Logistic Regression', 'KNN'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'Logistic Regression', 'Decision Tree'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'Logistic Regression', 'GaussianNB'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'Logistic Regression', 'Random Forest'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'Logistic Regression', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'LDA', 'KNN'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'LDA', 'Decision Tree'] stacked: mean: 0.9164335664335665, std: 0.052890643218602304\n",
      "['SVC', 'LDA', 'GaussianNB'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'LDA', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['SVC', 'LDA', 'XGBoost'] stacked: mean: 0.9164335664335665, std: 0.052890643218602304\n",
      "['SVC', 'KNN', 'Decision Tree'] stacked: mean: 0.9318181818181819, std: 0.06283770107069006\n",
      "['SVC', 'KNN', 'GaussianNB'] stacked: mean: 0.9331002331002332, std: 0.06258035841187393\n",
      "['SVC', 'KNN', 'Random Forest'] stacked: mean: 0.9318181818181819, std: 0.06283770107069006\n",
      "['SVC', 'KNN', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9315850815850816, std: 0.06710655726354578\n",
      "['SVC', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['SVC', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['SVC', 'GaussianNB', 'Random Forest'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9331002331002332, std: 0.06258035841187393\n",
      "['SVC', 'Random Forest', 'XGBoost'] stacked: mean: 0.9331002331002332, std: 0.06258035841187393\n",
      "['Logistic Regression', 'LDA', 'KNN'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'LDA', 'Decision Tree'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['Logistic Regression', 'LDA', 'GaussianNB'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'LDA', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'LDA', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Logistic Regression', 'KNN', 'Decision Tree'] stacked: mean: 0.9303030303030303, std: 0.06731775724703956\n",
      "['Logistic Regression', 'KNN', 'GaussianNB'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'KNN', 'Random Forest'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'KNN', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "['Logistic Regression', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Logistic Regression', 'Random Forest', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['LDA', 'KNN', 'Decision Tree'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'KNN', 'GaussianNB'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'KNN', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'KNN', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['LDA', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['LDA', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['LDA', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['LDA', 'Random Forest', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['KNN', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['KNN', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['KNN', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "['KNN', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['KNN', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['KNN', 'Random Forest', 'XGBoost'] stacked: mean: 1.0, std: 0.0\n",
      "['Decision Tree', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Decision Tree', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Decision Tree', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['GaussianNB', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n"
     ]
    }
   ],
   "source": [
    "stacking(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking, here, increasres the model performance even further than bagging. Indeed, the combination of KNN, Random Forest and XGBoost (which itself is a boosting algorithm) achieves 100% accuracy. We also try different numbers of classifiers to be stacked simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SVC', 'Logistic Regression'] stacked: mean: 0.9331002331002332, std: 0.06258035841187393\n",
      "['SVC', 'LDA'] stacked: mean: 0.8785547785547786, std: 0.1357804355865562\n",
      "['SVC', 'KNN'] stacked: mean: 0.8785547785547786, std: 0.1357804355865562\n",
      "['SVC', 'Decision Tree'] stacked: mean: 0.95, std: 0.06666666666666667\n",
      "['SVC', 'GaussianNB'] stacked: mean: 0.9164335664335665, std: 0.052890643218602304\n",
      "['SVC', 'Random Forest'] stacked: mean: 0.9315850815850816, std: 0.06710655726354578\n",
      "['SVC', 'XGBoost'] stacked: mean: 0.9164335664335663, std: 0.052890643218602304\n",
      "['Logistic Regression', 'LDA'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'KNN'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'Decision Tree'] stacked: mean: 1.0, std: 0.0\n",
      "['Logistic Regression', 'GaussianNB'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['LDA', 'KNN'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'Decision Tree'] stacked: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "['LDA', 'GaussianNB'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['LDA', 'Random Forest'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['LDA', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['KNN', 'Decision Tree'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['KNN', 'GaussianNB'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['KNN', 'Random Forest'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['KNN', 'XGBoost'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Decision Tree', 'GaussianNB'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Decision Tree', 'Random Forest'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['Decision Tree', 'XGBoost'] stacked: mean: 0.9833333333333332, std: 0.03333333333333335\n",
      "['GaussianNB', 'Random Forest'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['GaussianNB', 'XGBoost'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Random Forest', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n"
     ]
    }
   ],
   "source": [
    "stacking(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SVC', 'Logistic Regression', 'LDA', 'KNN'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'Logistic Regression', 'LDA', 'Decision Tree'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'Logistic Regression', 'LDA', 'GaussianNB'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'Logistic Regression', 'LDA', 'Random Forest'] stacked: mean: 0.9315850815850816, std: 0.06710655726354578\n",
      "['SVC', 'Logistic Regression', 'LDA', 'XGBoost'] stacked: mean: 0.9303030303030303, std: 0.08549536957373262\n",
      "['SVC', 'Logistic Regression', 'KNN', 'Decision Tree'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['SVC', 'Logistic Regression', 'KNN', 'GaussianNB'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'Logistic Regression', 'KNN', 'Random Forest'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['SVC', 'Logistic Regression', 'KNN', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['SVC', 'Logistic Regression', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9303030303030303, std: 0.08549536957373262\n",
      "['SVC', 'Logistic Regression', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['SVC', 'Logistic Regression', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "['SVC', 'Logistic Regression', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['SVC', 'Logistic Regression', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'Logistic Regression', 'Random Forest', 'XGBoost'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'LDA', 'KNN', 'Decision Tree'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'LDA', 'KNN', 'GaussianNB'] stacked: mean: 0.914918414918415, std: 0.07817724500234807\n",
      "['SVC', 'LDA', 'KNN', 'Random Forest'] stacked: mean: 0.9331002331002332, std: 0.06258035841187393\n",
      "['SVC', 'LDA', 'KNN', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'LDA', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9482517482517483, std: 0.0715340523219268\n",
      "['SVC', 'LDA', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9679487179487178, std: 0.039306947991681766\n",
      "['SVC', 'LDA', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9331002331002332, std: 0.06258035841187393\n",
      "['SVC', 'LDA', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'LDA', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'LDA', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'KNN', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'KNN', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'KNN', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'KNN', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'KNN', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9484848484848485, std: 0.06748805288278813\n",
      "['SVC', 'KNN', 'Random Forest', 'XGBoost'] stacked: mean: 0.9497668997668998, std: 0.04125347312053117\n",
      "['SVC', 'Decision Tree', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'Decision Tree', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'Decision Tree', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['SVC', 'GaussianNB', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Logistic Regression', 'LDA', 'KNN', 'Decision Tree'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'LDA', 'KNN', 'GaussianNB'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'LDA', 'KNN', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'LDA', 'KNN', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Logistic Regression', 'LDA', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'LDA', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['Logistic Regression', 'LDA', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['Logistic Regression', 'LDA', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'LDA', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'LDA', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Logistic Regression', 'KNN', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest'] stacked: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "['Logistic Regression', 'KNN', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'KNN', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'KNN', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'KNN', 'Random Forest', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['Logistic Regression', 'Decision Tree', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['Logistic Regression', 'Decision Tree', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost'] stacked: mean: 0.9484848484848485, std: 0.04215281134316231\n",
      "['Logistic Regression', 'GaussianNB', 'Random Forest', 'XGBoost'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'KNN', 'Decision Tree', 'GaussianNB'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'KNN', 'Decision Tree', 'Random Forest'] stacked: mean: 1.0, std: 0.0\n",
      "['LDA', 'KNN', 'Decision Tree', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['LDA', 'KNN', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'KNN', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['LDA', 'KNN', 'Random Forest', 'XGBoost'] stacked: mean: 0.9303030303030303, std: 0.06731775724703956\n",
      "['LDA', 'Decision Tree', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['LDA', 'Decision Tree', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9636363636363636, std: 0.07272727272727271\n",
      "['LDA', 'Decision Tree', 'Random Forest', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n",
      "['LDA', 'GaussianNB', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['KNN', 'Decision Tree', 'GaussianNB', 'Random Forest'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['KNN', 'Decision Tree', 'GaussianNB', 'XGBoost'] stacked: mean: 0.9469696969696969, std: 0.0720294807515437\n",
      "['KNN', 'Decision Tree', 'Random Forest', 'XGBoost'] stacked: mean: 1.0, std: 0.0\n",
      "['KNN', 'GaussianNB', 'Random Forest', 'XGBoost'] stacked: mean: 0.9818181818181818, std: 0.036363636363636376\n",
      "['Decision Tree', 'GaussianNB', 'Random Forest', 'XGBoost'] stacked: mean: 0.9651515151515151, std: 0.04274768478686633\n"
     ]
    }
   ],
   "source": [
    "stacking(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance with two models to be stacked at once is slightly worse than stacking three, and stacking four exhibits similar performance, with 100% accuracy with the combination of KNN, Decision Tree, Random Forest, and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean scores are much higher than the default classifier case, and indeed the ensemble method case. Therefore we accept stacking as the best ensemble method characterised here. Because Table1 already has 100% accuracy, these results will not be integrated into the main models. Nevertheless, for the purposes of small dataset escalation methods, this notebook provides a detailed comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
