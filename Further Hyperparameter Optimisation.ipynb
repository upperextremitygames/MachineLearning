{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt Bayesian Hyperparameter Optimisation  - Table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook takes the data presented in Table2, and seeks an alternative form of hyperparameter optimisation. It was noted in the report that Bayesian optimisation can be an improvement over vanilla grid search and random search forms of hyperparameter optimisation, found within Scikit-learn. Bayesian optimisation is not found within Scikit-learn's API, however, so Hyperopt, an open-source Python library, was chosen. Hyperopt-sklearn is a python package that runs on top of this, although with hours of experimentation the API was not working, with unfixable bugs. Consequently, Hyperopt was used instead, with optimisation algorithms created from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the packages and data will be imported into this Jupyter notebook in the same way as in Table2, this process will be conducted without explaination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "# ignoring warnings, to make the results simpler to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and modifying tables\n",
    "new = pd.read_excel('Data2.xlsx', sheet_name = 'Table 2 - Games')\n",
    "new.set_index('Patient No. (ID)', inplace=True)\n",
    "new.drop(['# Possible Targets', 'Targets hit', 'Time per target / overall'], axis=1, inplace=True)\n",
    "new.rename(columns={'Average hit time (s) (for successful hits)': 'Average hit time', 'Total time taken (s)': 'Time taken'}, inplace=True)\n",
    "new = new.round({'Time taken' : 0, '% targets hit': 2, 'Average hit time' : 1})\n",
    "easy_filter = new['Difficulty'] == 'Easy'\n",
    "easy = new[easy_filter]\n",
    "medium_filter = new['Difficulty'] == 'Medium'\n",
    "medium = new[medium_filter]\n",
    "hard_filter = new['Difficulty'] == 'Hard'\n",
    "hard = new[hard_filter]\n",
    "easy.drop(['Difficulty'], axis=1, inplace=True)\n",
    "medium.drop(['Difficulty'], axis=1, inplace=True)\n",
    "hard.drop(['Difficulty'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_easy = easy.drop(columns=['Output'])\n",
    "y_easy = easy.Output\n",
    "X_train_easy, X_test_easy, y_train_easy, y_test_easy = train_test_split(X_easy, y_easy, test_size=0.2, random_state=13)\n",
    "\n",
    "X_medium = medium.drop(columns=['Output'])\n",
    "y_medium = medium.Output\n",
    "X_train_medium, X_test_medium, y_train_medium, y_test_medium = train_test_split(X_medium, y_medium, test_size=0.2, random_state=13)\n",
    "\n",
    "X_hard = hard.drop(columns=['Output'])\n",
    "y_hard = hard.Output\n",
    "X_train_hard, X_test_hard, y_train_hard, y_test_hard = train_test_split(X_hard, y_hard, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import Hyperopt, and the classification algorithms we are going to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the hyperparameter space. This looks similar to the way it was specified in the GridSearchCV space. This is deliberate, because we want a fair comparisation between the two. Note, however, that Hyperopt, rather than iterate through every combination of values, will select a random set of values initially, and then change the hyperparameter values, within the range provided, with the objective of maximising model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_RFC = {'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "                 'max_features': hp.choice('max_features', ['log2', 'sqrt','auto']),\n",
    "                 'n_estimators': hp.choice('n_estimators', range(100,500)),\n",
    "                 'criterion': hp.choice('criterion', ['gini', 'entropy']), \n",
    "                 'min_samples_split': hp.choice('min_samples_split', range(2,10)),\n",
    "                  'min_samples_leaf': hp.choice('min_samples_leaf', range(1,10))}\n",
    "parameters_XGBC = {'n_estimators': hp.choice('n_estimators', range(200, 800)), \n",
    "                  'early_stopping_rounds': hp.choice('early_stopping_rounds', range(2, 8)), \n",
    "                  'learning_rate': hp.uniform('learning_rate', 0.05, 0.5)\n",
    "                  } \n",
    "parameters_SVM = {'C' : hp.uniform('C', 0.01, 100),\n",
    "                 'kernel': hp.choice('kernel', ['linear', 'rbf', 'sigmoid']),\n",
    "                  'gamma': hp.uniform('gamma', 0.001, 1)\n",
    "                 }\n",
    "parameters_decision_tree = {'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "                             'criterion': hp.choice('criterion', ['gini', 'entropy']), \n",
    "                             'min_samples_split': hp.choice('min_samples_split', range(2,10)),\n",
    "                              'min_samples_leaf': hp.choice('min_samples_leaf', range(1,10))}\n",
    "parameters_NB = {'priors' : hp.choice('priors', [None])}\n",
    "parameters_kNN = {'n_neighbors': hp.choice('n_neighbors', range(2, 30)),\n",
    "                 'p': hp.choice('p', range(1, 6))}\n",
    "parameters_LDA = {'solver': hp.choice('solver', ['svd', 'lsqr', 'eigen'])}\n",
    "parameters_logistic = {'C': hp.uniform('C', 0.01, 10000),\n",
    "                      'penalty': hp.choice('penalty', ['l1', 'l2'])}\n",
    "\n",
    "models = {'SVM': [SVC(), parameters_SVM],\n",
    "          'Logistic': [LogisticRegression(), parameters_logistic],\n",
    "          'LDA': [LinearDiscriminantAnalysis(), parameters_LDA],\n",
    "          'kNN': [KNeighborsClassifier(), parameters_kNN],\n",
    "          'Decision Tree': [DecisionTreeClassifier(), parameters_decision_tree],\n",
    "          'Naive Bayes' : [GaussianNB(), parameters_NB],\n",
    "        'Random Forest': [RandomForestClassifier(), parameters_RFC],\n",
    "         'XGBoost': [XGBClassifier(), parameters_XGBC]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant functions necessary for Hyperopt have been created, modified from the functions in the GridSearchCV case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params):\n",
    "    global best\n",
    "    acc=cross_val_score(a, b, c, cv=4).mean()\n",
    "    if acc > best:\n",
    "            best = acc\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "    \n",
    "def best_classifier(diff, models):    \n",
    "    global results, a, b, c, best\n",
    "    results = {}\n",
    "    results['Easy'] = {}\n",
    "    results['Medium'] = {}\n",
    "    results['Hard'] = {}\n",
    "    for j in diff.keys():\n",
    "        for i in models.keys():\n",
    "            best = 0\n",
    "            trials=Trials()\n",
    "            a = models[i][0]\n",
    "            b = diff[j][0]\n",
    "            c = diff[j][1]\n",
    "            best_result = fmin(f, models[i][1], algo=tpe.suggest, max_evals=250, trials=trials)\n",
    "            print(f'{i} on table {j} scored {best}')\n",
    "            results[j][i] = [best, best_result]\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "diff = {'Easy': [X_train_easy, y_train_easy], 'Medium': [X_train_medium, y_train_easy], 'Hard': [X_train_hard, y_train_hard]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the models are run and trained, each one 250 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 250/250 [00:06<00:00, 36.24it/s, best loss: -0.8999999999999999]\n",
      "SVM on table Easy scored 0.8999999999999999\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:04<00:00, 50.88it/s, best loss: -0.5]\n",
      "Logistic on table Easy scored 0.5\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:04<00:00, 53.91it/s, best loss: -1.0]\n",
      "LDA on table Easy scored 1.0\n",
      "100%|██████████████████████████████████████████████████████████████| 250/250 [00:05<00:00, 48.15it/s, best loss: -0.95]\n",
      "kNN on table Easy scored 0.95\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:04<00:00, 53.46it/s, best loss: -1.0]\n",
      "Decision Tree on table Easy scored 1.0\n",
      "100%|████████████████████████████████████████████████| 250/250 [00:03<00:00, 63.38it/s, best loss: -0.8999999999999999]\n",
      "Naive Bayes on table Easy scored 0.8999999999999999\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 13.67it/s, best loss: -1.0]\n",
      "Random Forest on table Easy scored 1.0\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:28<00:00,  8.10it/s, best loss: -1.0]\n",
      "XGBoost on table Easy scored 1.0\n",
      "100%|████████████████████████████████████████████████| 250/250 [00:06<00:00, 38.34it/s, best loss: -0.7000000000000001]\n",
      "SVM on table Medium scored 0.7000000000000001\n",
      "100%|██████████████████████████████████████████████████████████████| 250/250 [00:04<00:00, 55.43it/s, best loss: -0.65]\n",
      "Logistic on table Medium scored 0.65\n",
      "100%|██████████████████████████████████████████████████████████████| 250/250 [00:04<00:00, 62.02it/s, best loss: -0.95]\n",
      "LDA on table Medium scored 0.95\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:05<00:00, 46.53it/s, best loss: -0.9]\n",
      "kNN on table Medium scored 0.9\n",
      "100%|██████████████████████████████████████████████████████████████| 250/250 [00:05<00:00, 45.12it/s, best loss: -0.95]\n",
      "Decision Tree on table Medium scored 0.95\n",
      "100%|████████████████████████████████████████████████| 250/250 [00:04<00:00, 54.62it/s, best loss: -0.8500000000000001]\n",
      "Naive Bayes on table Medium scored 0.8500000000000001\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:18<00:00, 13.85it/s, best loss: -1.0]\n",
      "Random Forest on table Medium scored 1.0\n",
      "100%|███████████████████████████████████████████████████████████████| 250/250 [00:27<00:00,  9.15it/s, best loss: -0.9]\n",
      "XGBoost on table Medium scored 0.9\n",
      "100%|████████████████████████████████████████████████| 250/250 [00:05<00:00, 41.36it/s, best loss: -0.7000000000000001]\n",
      "SVM on table Hard scored 0.7000000000000001\n",
      "100%|███████████████████████████████████████████████| 250/250 [00:04<00:00, 57.58it/s, best loss: -0.45000000000000007]\n",
      "Logistic on table Hard scored 0.45000000000000007\n",
      "100%|██████████████████████████████████████████████████████████████| 250/250 [00:04<00:00, 61.03it/s, best loss: -0.75]\n",
      "LDA on table Hard scored 0.75\n",
      "100%|██████████████████████████████████████████████████████████████| 250/250 [00:05<00:00, 49.05it/s, best loss: -0.95]\n",
      "kNN on table Hard scored 0.95\n",
      "100%|████████████████████████████████████████████████| 250/250 [00:05<00:00, 47.90it/s, best loss: -0.8500000000000001]\n",
      "Decision Tree on table Hard scored 0.8500000000000001\n",
      "100%|████████████████████████████████████████████████| 250/250 [00:04<00:00, 57.85it/s, best loss: -0.7500000000000001]\n",
      "Naive Bayes on table Hard scored 0.7500000000000001\n",
      "100%|██████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.09it/s, best loss: -0.95]\n",
      "Random Forest on table Hard scored 0.95\n",
      "100%|████████████████████████████████████████████████| 250/250 [00:28<00:00,  8.65it/s, best loss: -0.8500000000000001]\n",
      "XGBoost on table Hard scored 0.8500000000000001\n",
      "{'Easy': {'SVM': [0.8999999999999999, {'C': 28, 'gamma': 36, 'kernel': 2}], 'Logistic': [0.5, {'C': 4, 'penalty': 0}], 'LDA': [1.0, {'solver': 1}], 'kNN': [0.95, {'n_neighbors': 9, 'p': 0}], 'Decision Tree': [1.0, {'criterion': 1, 'max_depth': 5, 'min_samples_leaf': 6, 'min_samples_split': 2}], 'Naive Bayes': [0.8999999999999999, {'priors': 0}], 'Random Forest': [1.0, {'criterion': 1, 'max_depth': 15, 'max_features': 2, 'min_samples_leaf': 0, 'min_samples_split': 5, 'n_estimators': 239}], 'XGBoost': [1.0, {'early_stopping_rounds': 0, 'learning_rate': 48, 'n_estimators': 419}]}, 'Medium': {'SVM': [0.7000000000000001, {'C': 5, 'gamma': 23, 'kernel': 1}], 'Logistic': [0.65, {'C': 1, 'penalty': 1}], 'LDA': [0.95, {'solver': 1}], 'kNN': [0.9, {'n_neighbors': 25, 'p': 3}], 'Decision Tree': [0.95, {'criterion': 0, 'max_depth': 3, 'min_samples_leaf': 6, 'min_samples_split': 3}], 'Naive Bayes': [0.8500000000000001, {'priors': 0}], 'Random Forest': [1.0, {'criterion': 0, 'max_depth': 9, 'max_features': 0, 'min_samples_leaf': 0, 'min_samples_split': 3, 'n_estimators': 392}], 'XGBoost': [0.9, {'early_stopping_rounds': 4, 'learning_rate': 8, 'n_estimators': 446}]}, 'Hard': {'SVM': [0.7000000000000001, {'C': 6, 'gamma': 35, 'kernel': 0}], 'Logistic': [0.45000000000000007, {'C': 0, 'penalty': 1}], 'LDA': [0.75, {'solver': 2}], 'kNN': [0.95, {'n_neighbors': 18, 'p': 2}], 'Decision Tree': [0.8500000000000001, {'criterion': 1, 'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 3}], 'Naive Bayes': [0.7500000000000001, {'priors': 0}], 'Random Forest': [0.95, {'criterion': 0, 'max_depth': 18, 'max_features': 0, 'min_samples_leaf': 8, 'min_samples_split': 4, 'n_estimators': 189}], 'XGBoost': [0.8500000000000001, {'early_stopping_rounds': 4, 'learning_rate': 16, 'n_estimators': 42}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Easy': {'SVM': [0.8999999999999999, {'C': 28, 'gamma': 36, 'kernel': 2}],\n",
       "  'Logistic': [0.5, {'C': 4, 'penalty': 0}],\n",
       "  'LDA': [1.0, {'solver': 1}],\n",
       "  'kNN': [0.95, {'n_neighbors': 9, 'p': 0}],\n",
       "  'Decision Tree': [1.0,\n",
       "   {'criterion': 1,\n",
       "    'max_depth': 5,\n",
       "    'min_samples_leaf': 6,\n",
       "    'min_samples_split': 2}],\n",
       "  'Naive Bayes': [0.8999999999999999, {'priors': 0}],\n",
       "  'Random Forest': [1.0,\n",
       "   {'criterion': 1,\n",
       "    'max_depth': 15,\n",
       "    'max_features': 2,\n",
       "    'min_samples_leaf': 0,\n",
       "    'min_samples_split': 5,\n",
       "    'n_estimators': 239}],\n",
       "  'XGBoost': [1.0,\n",
       "   {'early_stopping_rounds': 0, 'learning_rate': 48, 'n_estimators': 419}]},\n",
       " 'Medium': {'SVM': [0.7000000000000001, {'C': 5, 'gamma': 23, 'kernel': 1}],\n",
       "  'Logistic': [0.65, {'C': 1, 'penalty': 1}],\n",
       "  'LDA': [0.95, {'solver': 1}],\n",
       "  'kNN': [0.9, {'n_neighbors': 25, 'p': 3}],\n",
       "  'Decision Tree': [0.95,\n",
       "   {'criterion': 0,\n",
       "    'max_depth': 3,\n",
       "    'min_samples_leaf': 6,\n",
       "    'min_samples_split': 3}],\n",
       "  'Naive Bayes': [0.8500000000000001, {'priors': 0}],\n",
       "  'Random Forest': [1.0,\n",
       "   {'criterion': 0,\n",
       "    'max_depth': 9,\n",
       "    'max_features': 0,\n",
       "    'min_samples_leaf': 0,\n",
       "    'min_samples_split': 3,\n",
       "    'n_estimators': 392}],\n",
       "  'XGBoost': [0.9,\n",
       "   {'early_stopping_rounds': 4, 'learning_rate': 8, 'n_estimators': 446}]},\n",
       " 'Hard': {'SVM': [0.7000000000000001, {'C': 6, 'gamma': 35, 'kernel': 0}],\n",
       "  'Logistic': [0.45000000000000007, {'C': 0, 'penalty': 1}],\n",
       "  'LDA': [0.75, {'solver': 2}],\n",
       "  'kNN': [0.95, {'n_neighbors': 18, 'p': 2}],\n",
       "  'Decision Tree': [0.8500000000000001,\n",
       "   {'criterion': 1,\n",
       "    'max_depth': 6,\n",
       "    'min_samples_leaf': 2,\n",
       "    'min_samples_split': 3}],\n",
       "  'Naive Bayes': [0.7500000000000001, {'priors': 0}],\n",
       "  'Random Forest': [0.95,\n",
       "   {'criterion': 0,\n",
       "    'max_depth': 18,\n",
       "    'max_features': 0,\n",
       "    'min_samples_leaf': 8,\n",
       "    'min_samples_split': 4,\n",
       "    'n_estimators': 189}],\n",
       "  'XGBoost': [0.8500000000000001,\n",
       "   {'early_stopping_rounds': 4, 'learning_rate': 16, 'n_estimators': 42}]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier(diff, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean performance of each model is then calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average():\n",
    "    averages = {}\n",
    "    for i in results.keys():\n",
    "        for j in results[i].keys():\n",
    "            averages[j] = 0\n",
    "    for i in results.keys():\n",
    "        for j in results[i].keys():\n",
    "            averages[j] += results[i][j][0]\n",
    "    for x in averages.keys():\n",
    "        averages[x] /= 3\n",
    "    print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM': 0.7666666666666667, 'Logistic': 0.5333333333333333, 'LDA': 0.9, 'kNN': 0.9333333333333332, 'Decision Tree': 0.9333333333333332, 'Naive Bayes': 0.8333333333333334, 'Random Forest': 0.9833333333333334, 'XGBoost': 0.9166666666666666}\n"
     ]
    }
   ],
   "source": [
    "find_average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Random Forest appears to perform best. It has an identical score to hyperparameter optimisation using Grid Search, moreover, so this does not change our optimal model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
