{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt Bayesian Hyperparameter Optimisation  - Table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook takes the data presented in Table2, and seeks an alternative form of hyperparameter optimisation. It was noted in the report that Bayesian optimisation can be an improvement over vanilla grid search and random search forms of hyperparameter optimisation, found within Scikit-learn. Bayesian optimisation is not found within Scikit-learn's API, however, so Hyperopt, an open-source Python library, was chosen. Hyperopt-sklearn is a python package that runs on top of this, although with hours of experimentation the API was not working, with unfixable bugs. Consequently, Hyperopt was used instead, with optimisation algorithms created from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the packages and data will be imported into this Jupyter notebook in the same way as in Table2, this process will be conducted without explaination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "# ignoring warnings, to make the results simpler to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and modifying tables\n",
    "new = pd.read_excel('Data2.xlsx', sheet_name = 'Table 2 - Games')\n",
    "new.set_index('Patient No. (ID)', inplace=True)\n",
    "new.drop(['# Possible Targets', 'Targets hit', 'Time per target / overall'], axis=1, inplace=True)\n",
    "new.rename(columns={'Average hit time (s) (for successful hits)': 'Average hit time', 'Total time taken (s)': 'Time taken'}, inplace=True)\n",
    "new = new.round({'Time taken' : 0, '% targets hit': 2, 'Average hit time' : 1})\n",
    "easy_filter = new['Difficulty'] == 'Easy'\n",
    "easy = new[easy_filter]\n",
    "medium_filter = new['Difficulty'] == 'Medium'\n",
    "medium = new[medium_filter]\n",
    "hard_filter = new['Difficulty'] == 'Hard'\n",
    "hard = new[hard_filter]\n",
    "easy.drop(['Difficulty'], axis=1, inplace=True)\n",
    "medium.drop(['Difficulty'], axis=1, inplace=True)\n",
    "hard.drop(['Difficulty'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_easy = easy.drop(columns=['Output'])\n",
    "y_easy = easy.Output\n",
    "X_train_easy, X_test_easy, y_train_easy, y_test_easy = train_test_split(X_easy, y_easy, test_size=0.2, random_state=13)\n",
    "\n",
    "X_medium = medium.drop(columns=['Output'])\n",
    "y_medium = medium.Output\n",
    "X_train_medium, X_test_medium, y_train_medium, y_test_medium = train_test_split(X_medium, y_medium, test_size=0.2, random_state=13)\n",
    "\n",
    "X_hard = hard.drop(columns=['Output'])\n",
    "y_hard = hard.Output\n",
    "X_train_hard, X_test_hard, y_train_hard, y_test_hard = train_test_split(X_hard, y_hard, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by re-running the original gridsearchCV, and output the total time taken to train each algorithm, as well as the total time taken to run the entire grid search. Note that we do not care about the results, as these are already generated in Table2. Instead, we simply print the total time taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_RFC = {'n_estimators': [4, 6, 9], \n",
    "                  'max_features': ['log2', 'sqrt','auto'], \n",
    "                  'criterion': ['entropy', 'gini'],\n",
    "                  'max_depth': [2, 3, 5, 10], \n",
    "                  'min_samples_split': [2, 3, 5],\n",
    "                  'min_samples_leaf': [1,5,8]\n",
    "                 }\n",
    "parameters_XGBC = {'n_estimators': [400, 600, 800], \n",
    "                  'early_stopping_rounds': [3, 5, 7], \n",
    "                  'learning_rate': [0.05, 0.1, 0.3, 0.5]\n",
    "                  } \n",
    "parameters_SVM = {'C' : [0.01, 0.1, 1, 10, 100],\n",
    "                 'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "                  'gamma': [0.001, 0.01, 0.1, 1]\n",
    "                 }\n",
    "parameters_decision_tree = {'criterion': ['entropy', 'gini'],\n",
    "                             'max_depth': [2, 3, 5, 10],\n",
    "                            'min_samples_split': [2, 3, 5],\n",
    "                              'min_samples_leaf': [1,5,8]\n",
    "                           }\n",
    "parameters_NB = {'priors' : [None]}\n",
    "parameters_kNN = {'n_neighbors': [3, 5, 10, 15],\n",
    "                 'p': [1, 2, 3, 4]}\n",
    "parameters_LDA = {'solver': ['svd', 'lsqr', 'eigen']}\n",
    "parameters_logistic = {'C': np.logspace(-3, 3, 7),\n",
    "                      'penalty': ['l1', 'l2']}\n",
    "\n",
    "models = {'SVM': [SVC(), parameters_SVM],\n",
    "          'Logistic': [LogisticRegression(), parameters_logistic],\n",
    "          'LDA': [LinearDiscriminantAnalysis(), parameters_LDA],\n",
    "          'kNN': [KNeighborsClassifier(), parameters_kNN],\n",
    "          'Decision Tree': [DecisionTreeClassifier(), parameters_decision_tree],\n",
    "          'Naive Bayes' : [GaussianNB(), parameters_NB],\n",
    "        'Random Forest': [RandomForestClassifier(), parameters_RFC],\n",
    "         'XGBoost': [XGBClassifier(), parameters_XGBC]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_classifier(diff, models):\n",
    "    global results\n",
    "    results = {}\n",
    "    results['Easy'] = {}\n",
    "    results['Medium'] = {}\n",
    "    results['Hard'] = {}\n",
    "    for j in diff.keys():\n",
    "        for i in models:\n",
    "            start = time.time()\n",
    "            CV = GridSearchCV(models[i][0], models[i][1], cv=4, iid=False, scoring='accuracy', n_jobs=1)\n",
    "            CV = CV.fit(diff[j][0], diff[j][1])\n",
    "            end = time.time()\n",
    "            time_taken = end - start\n",
    "            print(f'{i} on table {j} took {time_taken:.2f}s')\n",
    "            results[j][i] = [CV.best_score_, CV.best_estimator_, time_taken]\n",
    "    return results\n",
    "diff = {'Easy': [X_train_easy, y_train_easy], 'Medium': [X_train_medium, y_train_easy], 'Hard': [X_train_hard, y_train_hard]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on table Easy took 0.92s\n",
      "Logistic on table Easy took 0.29s\n",
      "LDA on table Easy took 0.06s\n",
      "kNN on table Easy took 0.32s\n",
      "Decision Tree on table Easy took 1.07s\n",
      "Naive Bayes on table Easy took 0.02s\n",
      "Random Forest on table Easy took 27.25s\n",
      "XGBoost on table Easy took 11.17s\n",
      "SVM on table Medium took 1.14s\n",
      "Logistic on table Medium took 0.43s\n",
      "LDA on table Medium took 0.06s\n",
      "kNN on table Medium took 0.34s\n",
      "Decision Tree on table Medium took 1.12s\n",
      "Naive Bayes on table Medium took 0.02s\n",
      "Random Forest on table Medium took 26.30s\n",
      "XGBoost on table Medium took 12.73s\n",
      "SVM on table Hard took 1.29s\n",
      "Logistic on table Hard took 0.43s\n",
      "LDA on table Hard took 0.07s\n",
      "kNN on table Hard took 0.45s\n",
      "Decision Tree on table Hard took 1.40s\n",
      "Naive Bayes on table Hard took 0.02s\n",
      "Random Forest on table Hard took 25.18s\n",
      "XGBoost on table Hard took 10.07s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Easy': {'SVM': [1.0,\n",
       "   SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False),\n",
       "   0.9165854454040527],\n",
       "  'Logistic': [0.95,\n",
       "   LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   0.29024410247802734],\n",
       "  'LDA': [1.0,\n",
       "   LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                 solver='svd', store_covariance=False, tol=0.0001),\n",
       "   0.05580735206604004],\n",
       "  'kNN': [1.0,\n",
       "   KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "              metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "              weights='uniform'),\n",
       "   0.32413387298583984],\n",
       "  'Decision Tree': [1.0,\n",
       "   DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=3,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   1.072162389755249],\n",
       "  'Naive Bayes': [0.8999999999999999,\n",
       "   GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "   0.019945383071899414],\n",
       "  'Random Forest': [1.0,\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "               max_depth=2, max_features='log2', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=5,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=9, n_jobs=None,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "   27.249410390853882],\n",
       "  'XGBoost': [1.0,\n",
       "   XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=3,\n",
       "          gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "          min_child_weight=1, missing=None, n_estimators=400, n_jobs=1,\n",
       "          nthread=None, objective='multi:softprob', random_state=0,\n",
       "          reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "          silent=None, subsample=1, verbosity=1),\n",
       "   11.169788360595703]},\n",
       " 'Medium': {'SVM': [0.9,\n",
       "   SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False),\n",
       "   1.1385283470153809],\n",
       "  'Logistic': [0.9,\n",
       "   LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   0.42589378356933594],\n",
       "  'LDA': [0.95,\n",
       "   LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                 solver='svd', store_covariance=False, tol=0.0001),\n",
       "   0.055852413177490234],\n",
       "  'kNN': [0.9,\n",
       "   KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "              metric_params=None, n_jobs=None, n_neighbors=5, p=1,\n",
       "              weights='uniform'),\n",
       "   0.3420870304107666],\n",
       "  'Decision Tree': [0.95,\n",
       "   DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=3,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   1.122304916381836],\n",
       "  'Naive Bayes': [0.8500000000000001,\n",
       "   GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "   0.01894998550415039],\n",
       "  'Random Forest': [0.95,\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "               max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=3,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=6, n_jobs=None,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "   26.30358076095581],\n",
       "  'XGBoost': [0.9,\n",
       "   XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=3,\n",
       "          gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "          min_child_weight=1, missing=None, n_estimators=400, n_jobs=1,\n",
       "          nthread=None, objective='multi:softprob', random_state=0,\n",
       "          reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "          silent=None, subsample=1, verbosity=1),\n",
       "   12.72618556022644]},\n",
       " 'Hard': {'SVM': [0.95,\n",
       "   SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False),\n",
       "   1.2862539291381836],\n",
       "  'Logistic': [0.8500000000000001,\n",
       "   LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "             solver='warn', tol=0.0001, verbose=0, warm_start=False),\n",
       "   0.4308464527130127],\n",
       "  'LDA': [0.8500000000000001,\n",
       "   LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                 solver='eigen', store_covariance=False, tol=0.0001),\n",
       "   0.06981420516967773],\n",
       "  'kNN': [0.95,\n",
       "   KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "              metric_params=None, n_jobs=None, n_neighbors=5, p=1,\n",
       "              weights='uniform'),\n",
       "   0.450793981552124],\n",
       "  'Decision Tree': [0.8500000000000001,\n",
       "   DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=5,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   1.3983073234558105],\n",
       "  'Naive Bayes': [0.7500000000000001,\n",
       "   GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "   0.02297043800354004],\n",
       "  'Random Forest': [0.9,\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "               max_depth=2, max_features='log2', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=9, n_jobs=None,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "   25.179221868515015],\n",
       "  'XGBoost': [0.8500000000000001,\n",
       "   XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=3,\n",
       "          gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "          min_child_weight=1, missing=None, n_estimators=400, n_jobs=1,\n",
       "          nthread=None, objective='multi:softprob', random_state=0,\n",
       "          reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "          silent=None, subsample=1, verbosity=1),\n",
       "   10.067234992980957]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier(diff, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_time(results):\n",
    "    averages = {}\n",
    "    total = 0\n",
    "    for i in results.keys():\n",
    "        for j in results[i].keys():\n",
    "            averages[j] = 0\n",
    "    for i in results.keys():\n",
    "        for j in results[i].keys():\n",
    "            averages[j] += results[i][j][2]\n",
    "            total += results[i][j][2]\n",
    "    print(averages)\n",
    "    print('\\n' + f'Total time taken is {total:.2f}s')\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM': 3.341367721557617, 'Logistic': 1.146984338760376, 'LDA': 0.181473970413208, 'kNN': 1.1170148849487305, 'Decision Tree': 3.5927746295928955, 'Naive Bayes': 0.061865806579589844, 'Random Forest': 78.7322130203247, 'XGBoost': 33.9632089138031}\n",
      "\n",
      "Total time taken is 122.14s\n"
     ]
    }
   ],
   "source": [
    "total = find_total_time(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import Hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the hyperparameter space. This looks similar to the way it was specified in the GridSearchCV cace above. This is deliberate, because we want a fair comparisation between the two. Note, however, that Hyperopt, rather than iterate through every combination of values, will select a random set of values initially, and then change the hyperparameter values, within the range provided, with the objective of maximising model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_RFC = {'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "                 'max_features': hp.choice('max_features', ['log2', 'sqrt','auto']),\n",
    "                 'n_estimators': hp.choice('n_estimators', range(100,500)),\n",
    "                 'criterion': hp.choice('criterion', ['gini', 'entropy']), \n",
    "                 'min_samples_split': hp.choice('min_samples_split', range(2,10)),\n",
    "                  'min_samples_leaf': hp.choice('min_samples_leaf', range(1,10))}\n",
    "parameters_XGBC = {'n_estimators': hp.choice('n_estimators', range(200, 800)), \n",
    "                  'early_stopping_rounds': hp.choice('early_stopping_rounds', range(2, 8)), \n",
    "                  'learning_rate': hp.uniform('learning_rate', 0.05, 0.5)\n",
    "                  } \n",
    "parameters_SVM = {'C' : hp.uniform('C', 0.01, 100),\n",
    "                 'kernel': hp.choice('kernel', ['linear', 'rbf', 'sigmoid']),\n",
    "                  'gamma': hp.uniform('gamma', 0.001, 1)\n",
    "                 }\n",
    "parameters_decision_tree = {'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "                             'criterion': hp.choice('criterion', ['gini', 'entropy']), \n",
    "                             'min_samples_split': hp.choice('min_samples_split', range(2,10)),\n",
    "                              'min_samples_leaf': hp.choice('min_samples_leaf', range(1,10))}\n",
    "parameters_NB = {'priors' : hp.choice('priors', [None])}\n",
    "parameters_kNN = {'n_neighbors': hp.choice('n_neighbors', range(2, 30)),\n",
    "                 'p': hp.choice('p', range(1, 6))}\n",
    "parameters_LDA = {'solver': hp.choice('solver', ['svd', 'lsqr', 'eigen'])}\n",
    "parameters_logistic = {'C': hp.uniform('C', 0.01, 10000),\n",
    "                      'penalty': hp.choice('penalty', ['l1', 'l2'])}\n",
    "\n",
    "models = {'SVM': [SVC(), parameters_SVM],\n",
    "          'Logistic': [LogisticRegression(), parameters_logistic],\n",
    "          'LDA': [LinearDiscriminantAnalysis(), parameters_LDA],\n",
    "          'kNN': [KNeighborsClassifier(), parameters_kNN],\n",
    "          'Decision Tree': [DecisionTreeClassifier(), parameters_decision_tree],\n",
    "          'Naive Bayes' : [GaussianNB(), parameters_NB],\n",
    "        'Random Forest': [RandomForestClassifier(), parameters_RFC],\n",
    "         'XGBoost': [XGBClassifier(), parameters_XGBC]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant functions necessary for Hyperopt have been created, modified from the functions in the GridSearchCV case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params):\n",
    "    global best\n",
    "    acc=cross_val_score(a, b, c, cv=4).mean()\n",
    "    if acc > best:\n",
    "            best = acc\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "    \n",
    "def best_classifier(diff, models, max_evals):    \n",
    "    global results2, a, b, c, best\n",
    "    results2 = {}\n",
    "    results2['Easy'] = {}\n",
    "    results2['Medium'] = {}\n",
    "    results2['Hard'] = {}\n",
    "    for j in diff.keys():\n",
    "        for i in models.keys():\n",
    "            best = 0\n",
    "            trials=Trials()\n",
    "            a = models[i][0]\n",
    "            b = diff[j][0]\n",
    "            c = diff[j][1]\n",
    "            start = time.time()\n",
    "            best_result = fmin(f, models[i][1], algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "            end = time.time()\n",
    "            time_taken = end - start\n",
    "            print(f'{i} on table {j} scored {best:.3f} and took {(time_taken):.2f}s')\n",
    "            results2[j][i] = [best, best_result, time_taken]\n",
    "    print(results2)\n",
    "    return results2\n",
    "\n",
    "diff = {'Easy': [X_train_easy, y_train_easy], 'Medium': [X_train_medium, y_train_easy], 'Hard': [X_train_hard, y_train_hard]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the models are run and trained, each one 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 53.55it/s, best loss: -0.8999999999999999]\n",
      "SVM on table Easy scored 0.900 and took 0.90s\n",
      "100%|█████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 61.74it/s, best loss: -0.5]\n",
      "Logistic on table Easy scored 0.500 and took 0.82s\n",
      "100%|█████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 63.95it/s, best loss: -1.0]\n",
      "LDA on table Easy scored 1.000 and took 0.79s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 53.00it/s, best loss: -0.95]\n",
      "kNN on table Easy scored 0.950 and took 0.96s\n",
      "100%|█████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 56.73it/s, best loss: -1.0]\n",
      "Decision Tree on table Easy scored 1.000 and took 0.88s\n",
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 65.71it/s, best loss: -0.8999999999999999]\n",
      "Naive Bayes on table Easy scored 0.900 and took 0.77s\n",
      "100%|█████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.49it/s, best loss: -1.0]\n",
      "Random Forest on table Easy scored 1.000 and took 3.82s\n",
      "100%|█████████████████████████████████████████████████████████████████| 50/50 [00:05<00:00,  9.37it/s, best loss: -1.0]\n",
      "XGBoost on table Easy scored 1.000 and took 5.43s\n",
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 59.33it/s, best loss: -0.7000000000000001]\n",
      "SVM on table Medium scored 0.700 and took 0.85s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 61.51it/s, best loss: -0.65]\n",
      "Logistic on table Medium scored 0.650 and took 0.82s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 37.82it/s, best loss: -0.95]\n",
      "LDA on table Medium scored 0.950 and took 1.33s\n",
      "100%|█████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 44.96it/s, best loss: -0.9]\n",
      "kNN on table Medium scored 0.900 and took 1.13s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 57.23it/s, best loss: -0.95]\n",
      "Decision Tree on table Medium scored 0.950 and took 0.89s\n",
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 65.28it/s, best loss: -0.8500000000000001]\n",
      "Naive Bayes on table Medium scored 0.850 and took 0.77s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.28it/s, best loss: -0.95]\n",
      "Random Forest on table Medium scored 0.950 and took 3.82s\n",
      "100%|█████████████████████████████████████████████████████████████████| 50/50 [00:05<00:00,  9.01it/s, best loss: -0.9]\n",
      "XGBoost on table Medium scored 0.900 and took 5.52s\n",
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 56.33it/s, best loss: -0.7000000000000001]\n",
      "SVM on table Hard scored 0.700 and took 0.89s\n",
      "100%|█████████████████████████████████████████████████| 50/50 [00:00<00:00, 61.66it/s, best loss: -0.45000000000000007]\n",
      "Logistic on table Hard scored 0.450 and took 0.82s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 63.46it/s, best loss: -0.75]\n",
      "LDA on table Hard scored 0.750 and took 0.79s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 51.58it/s, best loss: -0.95]\n",
      "kNN on table Hard scored 0.950 and took 0.98s\n",
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 55.20it/s, best loss: -0.8500000000000001]\n",
      "Decision Tree on table Hard scored 0.850 and took 0.89s\n",
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 65.62it/s, best loss: -0.7500000000000001]\n",
      "Naive Bayes on table Hard scored 0.750 and took 0.77s\n",
      "100%|████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.81it/s, best loss: -0.95]\n",
      "Random Forest on table Hard scored 0.950 and took 4.35s\n",
      "100%|██████████████████████████████████████████████████| 50/50 [00:06<00:00,  9.24it/s, best loss: -0.8500000000000001]\n",
      "XGBoost on table Hard scored 0.850 and took 6.51s\n",
      "{'Easy': {'SVM': [0.8999999999999999, {'C': 79.58108840989637, 'gamma': 0.9682645046571987, 'kernel': 1}, 0.8976011276245117], 'Logistic': [0.5, {'C': 8920.560047175679, 'penalty': 0}, 0.817847490310669], 'LDA': [1.0, {'solver': 0}, 0.7878937721252441], 'kNN': [0.95, {'n_neighbors': 21, 'p': 4}, 0.9564435482025146], 'Decision Tree': [1.0, {'criterion': 1, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 5}, 0.8776247501373291], 'Naive Bayes': [0.8999999999999999, {'priors': 0}, 0.7669501304626465], 'Random Forest': [1.0, {'criterion': 1, 'max_depth': 18, 'max_features': 1, 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 357}, 3.8210830688476562], 'XGBoost': [1.0, {'early_stopping_rounds': 5, 'learning_rate': 0.37354141745490926, 'n_estimators': 436}, 5.425570964813232]}, 'Medium': {'SVM': [0.7000000000000001, {'C': 80.07012820627709, 'gamma': 0.12034864392429889, 'kernel': 1}, 0.849729061126709], 'Logistic': [0.65, {'C': 8429.13951987848, 'penalty': 1}, 0.8189077377319336], 'LDA': [0.95, {'solver': 2}, 1.3341245651245117], 'kNN': [0.9, {'n_neighbors': 21, 'p': 0}, 1.126033067703247], 'Decision Tree': [0.95, {'criterion': 1, 'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 4}, 0.8876454830169678], 'Naive Bayes': [0.8500000000000001, {'priors': 0}, 0.771935224533081], 'Random Forest': [0.95, {'criterion': 1, 'max_depth': 2, 'max_features': 0, 'min_samples_leaf': 7, 'min_samples_split': 6, 'n_estimators': 346}, 3.8163671493530273], 'XGBoost': [0.9, {'early_stopping_rounds': 0, 'learning_rate': 0.3566019621501957, 'n_estimators': 63}, 5.517340183258057]}, 'Hard': {'SVM': [0.7000000000000001, {'C': 97.31795611989372, 'gamma': 0.8786044765774407, 'kernel': 2}, 0.8946094512939453], 'Logistic': [0.45000000000000007, {'C': 2493.9776866287184, 'penalty': 1}, 0.8178160190582275], 'LDA': [0.75, {'solver': 0}, 0.7918834686279297], 'kNN': [0.95, {'n_neighbors': 15, 'p': 0}, 0.9813766479492188], 'Decision Tree': [0.8500000000000001, {'criterion': 0, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}, 0.8906195163726807], 'Naive Bayes': [0.7500000000000001, {'priors': 0}, 0.7679460048675537], 'Random Forest': [0.95, {'criterion': 0, 'max_depth': 18, 'max_features': 1, 'min_samples_leaf': 8, 'min_samples_split': 5, 'n_estimators': 276}, 4.354152679443359], 'XGBoost': [0.8500000000000001, {'early_stopping_rounds': 2, 'learning_rate': 0.1455849914459136, 'n_estimators': 267}, 6.513156414031982]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Easy': {'SVM': [0.8999999999999999,\n",
       "   {'C': 79.58108840989637, 'gamma': 0.9682645046571987, 'kernel': 1},\n",
       "   0.8976011276245117],\n",
       "  'Logistic': [0.5, {'C': 8920.560047175679, 'penalty': 0}, 0.817847490310669],\n",
       "  'LDA': [1.0, {'solver': 0}, 0.7878937721252441],\n",
       "  'kNN': [0.95, {'n_neighbors': 21, 'p': 4}, 0.9564435482025146],\n",
       "  'Decision Tree': [1.0,\n",
       "   {'criterion': 1,\n",
       "    'max_depth': 3,\n",
       "    'min_samples_leaf': 5,\n",
       "    'min_samples_split': 5},\n",
       "   0.8776247501373291],\n",
       "  'Naive Bayes': [0.8999999999999999, {'priors': 0}, 0.7669501304626465],\n",
       "  'Random Forest': [1.0,\n",
       "   {'criterion': 1,\n",
       "    'max_depth': 18,\n",
       "    'max_features': 1,\n",
       "    'min_samples_leaf': 3,\n",
       "    'min_samples_split': 3,\n",
       "    'n_estimators': 357},\n",
       "   3.8210830688476562],\n",
       "  'XGBoost': [1.0,\n",
       "   {'early_stopping_rounds': 5,\n",
       "    'learning_rate': 0.37354141745490926,\n",
       "    'n_estimators': 436},\n",
       "   5.425570964813232]},\n",
       " 'Medium': {'SVM': [0.7000000000000001,\n",
       "   {'C': 80.07012820627709, 'gamma': 0.12034864392429889, 'kernel': 1},\n",
       "   0.849729061126709],\n",
       "  'Logistic': [0.65,\n",
       "   {'C': 8429.13951987848, 'penalty': 1},\n",
       "   0.8189077377319336],\n",
       "  'LDA': [0.95, {'solver': 2}, 1.3341245651245117],\n",
       "  'kNN': [0.9, {'n_neighbors': 21, 'p': 0}, 1.126033067703247],\n",
       "  'Decision Tree': [0.95,\n",
       "   {'criterion': 1,\n",
       "    'max_depth': 6,\n",
       "    'min_samples_leaf': 3,\n",
       "    'min_samples_split': 4},\n",
       "   0.8876454830169678],\n",
       "  'Naive Bayes': [0.8500000000000001, {'priors': 0}, 0.771935224533081],\n",
       "  'Random Forest': [0.95,\n",
       "   {'criterion': 1,\n",
       "    'max_depth': 2,\n",
       "    'max_features': 0,\n",
       "    'min_samples_leaf': 7,\n",
       "    'min_samples_split': 6,\n",
       "    'n_estimators': 346},\n",
       "   3.8163671493530273],\n",
       "  'XGBoost': [0.9,\n",
       "   {'early_stopping_rounds': 0,\n",
       "    'learning_rate': 0.3566019621501957,\n",
       "    'n_estimators': 63},\n",
       "   5.517340183258057]},\n",
       " 'Hard': {'SVM': [0.7000000000000001,\n",
       "   {'C': 97.31795611989372, 'gamma': 0.8786044765774407, 'kernel': 2},\n",
       "   0.8946094512939453],\n",
       "  'Logistic': [0.45000000000000007,\n",
       "   {'C': 2493.9776866287184, 'penalty': 1},\n",
       "   0.8178160190582275],\n",
       "  'LDA': [0.75, {'solver': 0}, 0.7918834686279297],\n",
       "  'kNN': [0.95, {'n_neighbors': 15, 'p': 0}, 0.9813766479492188],\n",
       "  'Decision Tree': [0.8500000000000001,\n",
       "   {'criterion': 0,\n",
       "    'max_depth': 5,\n",
       "    'min_samples_leaf': 1,\n",
       "    'min_samples_split': 2},\n",
       "   0.8906195163726807],\n",
       "  'Naive Bayes': [0.7500000000000001, {'priors': 0}, 0.7679460048675537],\n",
       "  'Random Forest': [0.95,\n",
       "   {'criterion': 0,\n",
       "    'max_depth': 18,\n",
       "    'max_features': 1,\n",
       "    'min_samples_leaf': 8,\n",
       "    'min_samples_split': 5,\n",
       "    'n_estimators': 276},\n",
       "   4.354152679443359],\n",
       "  'XGBoost': [0.8500000000000001,\n",
       "   {'early_stopping_rounds': 2,\n",
       "    'learning_rate': 0.1455849914459136,\n",
       "    'n_estimators': 267},\n",
       "   6.513156414031982]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier(diff, models, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean performance of each model is then calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_score():\n",
    "    averages = {}\n",
    "    for i in results2.keys():\n",
    "        for j in results2[i].keys():\n",
    "            averages[j] = 0\n",
    "    for i in results2.keys():\n",
    "        for j in results2[i].keys():\n",
    "            averages[j] += results2[i][j][0]\n",
    "    for x in averages.keys():\n",
    "        averages[x] /= 3\n",
    "    print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM': 0.7666666666666667, 'Logistic': 0.5333333333333333, 'LDA': 0.9, 'kNN': 0.9333333333333332, 'Decision Tree': 0.9333333333333332, 'Naive Bayes': 0.8333333333333334, 'Random Forest': 0.9666666666666667, 'XGBoost': 0.9166666666666666}\n"
     ]
    }
   ],
   "source": [
    "find_average_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Random Forest appears to perform best. The accuracy scores for most models are very similar to the grid search case.\n",
    "What is more important here, however, is total time taken, for the efficiency of training is the major benefit of Bayesian Optimisation over grid search. Consequently, we output the total time taken, and difference and percentage difference compared to the grid search case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM': 2.641939640045166, 'Logistic': 2.45457124710083, 'LDA': 2.9139018058776855, 'kNN': 3.0638532638549805, 'Decision Tree': 2.6558897495269775, 'Naive Bayes': 2.3068313598632812, 'Random Forest': 11.991602897644043, 'XGBoost': 17.45606756210327}\n",
      "\n",
      "Total time taken is 45.48s\n"
     ]
    }
   ],
   "source": [
    "total2 = find_total_time(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Optimisation reduced training time by 76.65s, and by 62.76%\n"
     ]
    }
   ],
   "source": [
    "percentage_change = ((total2-total) / total) * 100\n",
    "print(f'Bayesian Optimisation reduced training time by {total-total2:.2f}s, and by {-percentage_change:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Bayesian Optimisation improves on training time significaintly, and produces similar results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
